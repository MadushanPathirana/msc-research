{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d16016-67cc-41b0-81f0-0d72e0c4438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from icecream import ic\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbbc9c-e693-4e7d-ae68-74043ef3b87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57886db5-79e6-41f6-8555-80682a409e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_now():\n",
    "  \n",
    "    return f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |\"\n",
    "#.strftime('%Y-%m-%d %H:%M:%S')\n",
    "ic.configureOutput(prefix=time_now)\n",
    "ic.configureOutput(includeContext=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b829d-c435-4692-b4a5-a08470226c17",
   "metadata": {},
   "source": [
    "https://github.com/jhljx/GKT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d9bf7-5e17-42b3-8946-b4fe4195decb",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/aggr/lstm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c27469-f6d1-402a-8e85-dcd8ea6f3afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt;\n",
    "np.random.seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52cfbf95-2e9a-45d1-987c-963469771833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score;\n",
    "from sklearn.metrics import classification_report;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03a4236-7f5c-40c5-ac9f-a43d2d8afac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8cbedf5-9e52-46a9-94bb-9bc357120fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('appmath.pkl')[['graph_id', 'user_id', \n",
    "       'goal_id',\n",
    "        'interaction_end_time',\n",
    "       'learning_objective_name', 'atom_id', \n",
    "       'correct', 'time_spent_answering_s', 'time_spent_on_instruction_s',\n",
    "       'goal_progress',\n",
    "       'target_status_and_progress', 'prev_concept_narrative',\n",
    "       'duration_s', 'is_target']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc1ea2be-8996-4520-b71e-e3be3c44e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('prereq_edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c33f922-d8a2-4ef2-b721-15632dc3002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "\n",
    "for _,i in df2.iterrows():    \n",
    "       \n",
    "        G.add_edge(i['source_lo_title'],i['dest_lo_title'])\n",
    "        \n",
    "subgraphs=[G.subgraph(i) for i in nx.connected_components(G)]\n",
    "\n",
    "for num,sub in enumerate(subgraphs):\n",
    "    \n",
    "    nx.set_node_attributes(sub,num,'subGraphId')\n",
    "\n",
    "nodeSubGraphId={}\n",
    "for g in subgraphs:\n",
    "    for node in g.nodes(data=True):\n",
    "         nodeSubGraphId[node[0]]= node[1]['subGraphId']\n",
    "            \n",
    "subgraphsNodeCountDict={ num: g.number_of_nodes() for num,g in  enumerate(subgraphs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee949d5e-eb06-4b71-8b4d-c1064e145cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype.name=='category':\n",
    "        df[col]=df[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f76eab-a75f-49b0-b846-5fc57abdc39a",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1jDiuseAdtZCYbAX2IRv2HU8zG5585bRi?usp=sharing#scrollTo=GQ9SKdThF8Bq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4da2d-53b7-457a-95fb-f4a05d6be4c4",
   "metadata": {},
   "source": [
    "https://anuradhawick.com/software-tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b4a39a-9905-44fd-8f8b-1c9d98468099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loGraphID']=df.learning_objective_name.apply(lambda x: nodeSubGraphId.get(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1439df-3e46-4644-9932-9f84ad5cc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty=df[~df.correct.isna()].groupby('atom_id')['correct'].mean().reset_index().rename(columns={'correct':'difficulty'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6585a193-339a-4bb8-b529-dce29b9befa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.merge(difficulty,how='left',on='atom_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a7f4b29-5915-44f8-ad35-8d7f5c8adaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loIndex={lo:i for i,lo in enumerate( df.learning_objective_name.unique())}\n",
    "#df['loIndex']=df['learning_objective_name'].apply(lambda x: loIndex[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e81eb600-b208-42ea-b014-8d24b27a13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomClassifier(x):\n",
    "    if x==None:\n",
    "        return 'learningMaterial'\n",
    "    else:\n",
    "        return 'question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f7b8a2-052c-47ac-bbfc-388de6cf03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['atom_type']=df.correct.apply(lambda x:atomClassifier(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50eec388-70ad-4200-acc0-fc21ab1ece6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['correctBinary']=df.correct*1\n",
    "df.atom_id=df.atom_id.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1980163b-c424-401d-8625-ebe39021d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['graph_id']=df2.apply(lambda x: nodeSubGraphId.get(x['source_lo_title']) if nodeSubGraphId.get(x['source_lo_title'])!=None else nodeSubGraphId.get(x['dest_lo_title']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781059f-735e-4ad4-8971-f0a6bfae0fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1504bbd-909c-490e-a86a-e05cf0447f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lo_graph_id_dic={}\n",
    "for i in df2.graph_id.unique():\n",
    "    unique_lo=np.unique(np.concatenate((df2[df2.graph_id==i]['source_lo_title'].unique(),((df2[df2.graph_id==i]['dest_lo_title'].unique())))))\n",
    "    unique_lo_graph_id_dic[i]={ item:int(num)  for  num,item in enumerate(unique_lo)}\n",
    "    df2.loc[df2.graph_id==i,'source_index' ]=df2.apply(lambda x: unique_lo_graph_id_dic.get(i).get(x['source_lo_title']),axis=1)\n",
    "    df2.loc[df2.graph_id==i,'dest_index' ]=df2.apply(lambda x: unique_lo_graph_id_dic.get(i).get(x['dest_lo_title']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1abebbe-ded8-4762-8693-4cf106abd73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.source_index=df2.source_index.astype(int)\n",
    "df2.dest_index=df2.dest_index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f67b688-9f2b-4651-a729-aaf4489bfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df.loGraphID.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7fe1c3c-8ce9-428f-9372-42617b842252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loIndex']=df.apply(lambda x: unique_lo_graph_id_dic[x['loGraphID']].get(x['learning_objective_name'] ),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f9e0c47-4ba3-4d61-8855-5a4db74f17c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_lo_id                    eda7ab6a-8070-4fd8-8387-da17a40fd99e\n",
       "dest_lo_id                      eda7ab6a-8070-4fd8-8387-da17a40fd99e\n",
       "source_lo_title    Write biconditional statements in symbolic for...\n",
       "dest_lo_title      Write biconditional statements in symbolic for...\n",
       "graph_id                                                           3\n",
       "source_index                                                      30\n",
       "dest_index                                                        30\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.graph_id==3].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "756d09ea-0894-458c-b0e3-41946de063aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loGraphID=26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efd8e210-131f-4918-a0b6-e9310541f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "loEdgeMapping=df2[df2['graph_id']==loGraphID][['source_index','dest_index']]\n",
    "loEdgeIndex=loEdgeMapping.values.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52de3b0c-123d-4c9c-9de8-f4a4bf26d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e16c203-4ad9-4538-b5b8-90c9cfdcf4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90676ceb-37e7-4fe3-a450-8902364626cf",
   "metadata": {},
   "source": [
    "usersGraphDataList=[]\n",
    "min_atom_count=5\n",
    "max_atom_count=500\n",
    "for user in tqdm(users): \n",
    "    try:\n",
    "        user_atom_count=df[(df.loGraphID==loGraphID) & (df.user_id==user) & (df.atom_type=='question')].shape[0]\n",
    "        if user_atom_count>min_atom_count and user_atom_count<=max_atom_count:\n",
    "            features, edges,target =user_dataset(df,df2,user,loGraphID )\n",
    "            data=generate_data(features, edges,target)\n",
    "            usersGraphDataList.append(data)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eacebd-2bcf-4c93-b1c5-282dd0a8dc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c34dad-cb42-4fc0-aa5b-8efbdb271cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "ff9e1b61-f0de-490f-9534-451a778b1e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1834\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn import GCNConv \n",
    "from torch_geometric.nn import SAGEConv, to_hetero ,LSTMAggregation\n",
    "import  torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d2ce58b-34e2-4d80-aa5b-583c2513a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels,requires_grad=False)\n",
    "        self.actfn1=nn.ReLU()\n",
    "        \n",
    "        self.conv2 = SAGEConv((-1, -1), hidden_channels,requires_grad=False)\n",
    "        self.actfn2=nn.ReLU()\n",
    "        \n",
    "        self.conv3 = SAGEConv((-1, -1), hidden_channels,requires_grad=False)\n",
    "        self.actfn3=nn.ReLU()\n",
    "        \n",
    "        self.linear=nn.Linear(hidden_channels,hidden_channels)\n",
    "        \n",
    "        #self.lstm = nn.LSTM(input_size=hidden_channels, hidden_size=50,num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.out=nn.Linear(hidden_channels,out_channels)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First Message Passing layer\n",
    "        x = self.actfn1(self.conv1(x, edge_index))\n",
    "        x= F.dropout(x,p=.1,training=self.training)\n",
    "        \n",
    "        # Second Message Passing layer\n",
    "        x = self.actfn2(self.conv2(x, edge_index))\n",
    "        x= F.dropout(x,p=.1,training=self.training)\n",
    "        \n",
    "        # Third Message Passing layer  \n",
    "        #x = self.actfn3(self.conv3(x, edge_index))\n",
    "        #x= F.dropout(x,p=.25,training=self.training)\n",
    "        x = self.linear(x)\n",
    "        #x, _ = self.lstm(x)\n",
    "       \n",
    "        \n",
    "        # Out layer\n",
    "        x=self.sigmoid( self.out(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "#model_1 = GNN(hidden_channels=64, out_channels=2)\n",
    "#model_1 = to_hetero(model_1, data.metadata(), aggr='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3b2fba3-b39f-4f06-b428-257bc737a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersGraphDataList\n",
    "def batch_generator(usersGraphDataList):\n",
    "    \n",
    "    np.random.shuffle(usersGraphDataList)\n",
    "    test_size=int(len(usersGraphDataList)*.2)\n",
    "    test_data=usersGraphDataList[:test_size]\n",
    "    train_data=usersGraphDataList[test_size:]\n",
    "    \n",
    "    return test_data,train_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1d24f-e3ee-48ef-854d-aca4d796d163",
   "metadata": {},
   "source": [
    "model_1 = GNN(hidden_channels=200, out_channels=2)\n",
    "model_1 = to_hetero(model_1, data.metadata(), aggr='mean')\n",
    "\n",
    "\n",
    "#model_2 = GAT(hidden_channels=100, out_channels=2)\n",
    "#model_2 = to_hetero(model_2, data.metadata(), aggr='mean')\n",
    "\n",
    "\n",
    "model=model_1\n",
    "#dataLoader=list(DataLoader(usersGraphDataList,batch_size=80))\n",
    "test_data,train_data=batch_generator(usersGraphDataList)\n",
    "learning_rate=0.01\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "train_acc_mean_list=[]\n",
    "test_acc_mean_list=[]\n",
    "loss_list=[]\n",
    "best_score=0\n",
    "\n",
    "start_training=True\n",
    "if start_training:\n",
    "    for epcoh in range(7):\n",
    "        train_acc_list=[]\n",
    "        test_acc_list=[]\n",
    "        loss_list=[]\n",
    "        for num, data in enumerate(tqdm(train_data)):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            #data.x = torch.from_numpy(data.x_dict)\n",
    "            #data.edge_index = torch.from_numpy(data.edge_index_dict)\n",
    "            out=model(data.x_dict, data.edge_index_dict)  # model1\n",
    "            #out = model(data.x, data.edge_index)\n",
    "            #BCELoss -> loss=criterion(out['atom'][data['atom'].train_mask].argmax(dim=1).float().requires_grad_(True),data['atom'].y[data['atom'].train_mask].float())\n",
    "            #loss=criterion(out['atom'][data['atom'].train_mask].argmax(dim=1).float().requires_grad_(True),data['atom'].y[data['atom'].train_mask].float())\n",
    "            loss=criterion(out['atom'],data['atom'].y)\n",
    "            #loss=criterion(out['atom'].argmax(dim=1).float().requires_grad_(True),data['atom'].y.float())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            pred=out['atom'].argmax(dim=1)\n",
    "            train_correct=pred==data['atom'].y\n",
    "            train_acc=int(train_correct.sum())/data['atom'].y.shape[0]\n",
    "            train_acc_list.append(train_acc)\n",
    "\n",
    "\n",
    "\n",
    "        #test_acc_mean_list.append(np.mean(test_acc_list))\n",
    "            if epcoh==0 and num ==0:\n",
    "                best_score=loss\n",
    "            elif best_score<loss:\n",
    "                best_score=loss\n",
    "                best_model = model\n",
    "            loss_list.append(loss.item())\n",
    "        model.eval()\n",
    "        model_performance='epoch : {} , loss :{:.4f} , accuracy : {:.4f} \\n'.format(epcoh+1,np.mean(loss_list),np.mean(train_acc_list))\n",
    "        ic(model_performance);\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4279fa0-67d5-4a71-8397-6d7411b793dd",
   "metadata": {},
   "source": [
    "test_score=[]\n",
    "target=[]\n",
    "pred_prob=[]\n",
    "for num, data in enumerate(test_data):\n",
    "    try:\n",
    "        out=model(data.x_dict, data.edge_index_dict)\n",
    "        pred=out['atom'].argmax(dim=1)\n",
    "        test_correct=pred[data['atom'].test_mask]==data['atom'].y[data['atom'].test_mask]\n",
    "        test_acc=int(test_correct.sum())/int(data['atom'].test_mask.sum())\n",
    "        print('Test case : {} , nodes : {} , accuracy : {:.4f} '.format(num+1,data['atom'].x.shape[0],test_acc))\n",
    "        target.append(data['atom'].y[data['atom'].test_mask].item())\n",
    "        pred_prob.append(out['atom'][data['atom'].test_mask].detach()[0].tolist()[1])\n",
    "        test_score.append(test_acc)\n",
    "    except:\n",
    "        pass\n",
    "print('Mean accuracy : {} , AUC : {}'.format(np.mean(test_score),roc_auc_score(np.array(target),pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4429acca-d9cc-4c6e-b5fd-e768bf551bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['graph_id', 'user_id', 'goal_id', 'interaction_end_time',\n",
       "       'learning_objective_name', 'atom_id', 'correct',\n",
       "       'time_spent_answering_s', 'time_spent_on_instruction_s',\n",
       "       'goal_progress', 'target_status_and_progress', 'prev_concept_narrative',\n",
       "       'duration_s', 'is_target', 'loGraphID', 'difficulty', 'atom_type',\n",
       "       'correctBinary', 'loIndex'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#user_dataset(df,df2,'0012ff2b-871a-45d6-8e33-804f7464fa0a',3 )\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de0bcc85-91bb-4241-8f0c-a439e98e3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=df[(df.loGraphID==loGraphID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ae39f4f-c0de-4b9b-8f43-6cd43f883cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dft.groupby('user_id')['atom_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "b5e28d04-f40e-4767-ae37-4868303047f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4fb849d4-3798-4524-a93a-fe7b56cfb90a',\n",
       " 'b660a384-1ca7-4cd8-9811-e2d3aedc378c',\n",
       " 'e1000e42-ffd1-4782-9f14-aa75ce6a8b0e',\n",
       " '37907049-c931-4ce8-82a8-94b6cd2c1dbd',\n",
       " 'f7086166-753c-4867-af57-0e2b82fe3cbf',\n",
       " 'e4ac6b18-5423-4886-b7ee-a8fd2c54ca9f',\n",
       " '9c40062e-0719-4f9b-9d6a-bca4d37a2ce5',\n",
       " 'e3bce254-2394-46f3-9053-8437e5b810a4',\n",
       " '3eb9212c-523a-49fb-a670-3a51ae1ff87e',\n",
       " '1014fea4-cd45-4b59-b903-4c59a4360fcb',\n",
       " 'bbc510e7-6659-4092-bde3-96d10fa0e7e0',\n",
       " '92a51cf7-0396-4751-8ac0-7e2ec2b4f02f',\n",
       " '7e0f991c-23b3-49bb-bcc5-38c26263bf4b',\n",
       " 'efb10a25-84d0-4e96-bd3d-762b24f1b0a1',\n",
       " 'ab380c7a-306e-4f3e-adf6-425bf2833a84',\n",
       " 'ef963abb-beda-4105-b142-72b6d0ca8b6f',\n",
       " 'c7e39c9c-39dc-42bd-a4ee-c622edecf3c9',\n",
       " '8259fae9-a618-46b6-a95a-380a41812f7b',\n",
       " '81f01061-9f19-4a2e-90b2-04c95bae81f2',\n",
       " '1c991ad0-89aa-4aa9-94b9-fea443b609bc',\n",
       " 'eb073db0-f433-4711-bb9f-c2941aca5273',\n",
       " 'ba36b8f6-e67d-4931-8149-2a2a5b78c101',\n",
       " '822068ff-5469-4321-b03c-b616645a4bb1',\n",
       " '905c2238-1c04-4c4a-b782-bccb21f5f967',\n",
       " 'dd102d86-6cb2-4684-bf5b-e0673381562d',\n",
       " 'd744d19c-13fd-488a-a7dc-fca89ea3614a',\n",
       " '7e574580-9081-4266-b59c-089ffc570e8e',\n",
       " '8e65ec8f-18eb-4546-85f2-5698aea5e4f1',\n",
       " 'f5cb8dea-0744-4f4a-ba73-405e51b89a45',\n",
       " 'a6bf16b1-45e7-4c2f-b823-e0edb7503a38']"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.groupby('user_id')['atom_id'].count().sort_values()[20:50].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "6b6069cc-a133-436a-9a8c-7caa893e2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_t=dft.groupby('user_id')['atom_id'].count().sort_values()[20:70].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "86ade587-eb2f-4a1e-90f4-92f720e3fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tu=dft[dft.user_id.isin(user_t)].sort_values(['user_id','interaction_end_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "0dcbbe89-ab03-4f23-b471-889bfc2333d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tu_list=[]\n",
    "question_count=0\n",
    "for row_id,row in df_tu.iterrows():\n",
    "    if row_id==0:\n",
    "        user_id=row['user_id']\n",
    "    if row_id>0:\n",
    "        if user_id != row['user_id']:\n",
    "            question_count=0\n",
    "            user_id=row['user_id']\n",
    "    if row['atom_type'] =='question':\n",
    "        question_count =question_count+1\n",
    "        if question_count>2:\n",
    "            df_tu_list.append(df_tu.iloc[:row_id+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b1a15-df4e-457a-a920-5b10b2f1c8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "3a9ef5b1-954f-497b-aea4-80f73f886f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1431"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5d70e-3fa7-4133-b1f7-b7f28ab2ca62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "afe4452c-16b3-4644-b09f-18854d7c7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_dataset_with_target_node(userDf,df2,user,goal,loEdgeIndex):\n",
    "    \n",
    "    '''\n",
    "    df - df should be filtered by goal id\n",
    "    \n",
    "    edges\n",
    "    -------\n",
    "    lo -> atom\n",
    "    atom -> atom\n",
    "    lo -> learning materials (lm)\n",
    "    atom -> lm\n",
    "    lm -> atom\n",
    "    lm -> lm\n",
    "    lm/atom -> target\n",
    "    \n",
    "    node / features\n",
    "    ---------------\n",
    "    \n",
    "    lo - radnom number\n",
    "    atom - dificulty ,  correctness\n",
    "    lm - time spend \n",
    "    '''\n",
    "    userDf.loc[userDf.shape[0]-1,'atom_type']='target'\n",
    "    \n",
    "    atomIndex=np.arange(userDf[userDf.atom_type=='question'].atom_id.shape[0])\n",
    "    lmIndex=np.arange(userDf[userDf.atom_type=='learningMaterial'].atom_id.shape[0])\n",
    "    \n",
    "    # assisgning questions and learningMaterials index seperatly in sinlge columns 'atom_index'\n",
    "    userDf.loc[userDf.atom_type=='question','atomIndex']=atomIndex\n",
    "    userDf.loc[userDf.atom_type=='learningMaterial','atomIndex']=lmIndex\n",
    "    userDf.loc[userDf.atom_type=='target','atomIndex']=0\n",
    "    \n",
    "    #lo-atom\n",
    "    loAtomEdgeIndex=userDf[userDf.atom_type=='question'][['loIndex','atomIndex']].values.transpose()\n",
    "    #lo-lm\n",
    "    loLmEdgeIndex=userDf[userDf.atom_type=='learningMaterial'][['loIndex','atomIndex']].values.transpose()\n",
    "    #lo-target\n",
    "    loTargetEdgeIndex=userDf[userDf.atom_type=='target'][['loIndex','atomIndex']].values.transpose()\n",
    "    \n",
    "    userDf['atom_type_shift']=userDf.atom_type.shift(-1)\n",
    "    userDf['atomIndex_shift']=userDf.atomIndex.shift(-1)\n",
    "    \n",
    "    featuresDict={}\n",
    "    #atom features\n",
    "    atomFeatures = userDf[userDf.atom_type=='question'][['difficulty','correctBinary']].astype(float).to_numpy()\n",
    "    featuresDict['atom']=atomFeatures\n",
    "    #learning materials features\n",
    "    #lmFeatures = userDf[userDf.atom_type=='learningMaterial'][['duration_s']].to_numpy()\n",
    "    \n",
    "    if  lmIndex.size>0 :\n",
    "            lmFeatures = userDf[userDf.atom_type=='learningMaterial'][['duration_s']].to_numpy()\n",
    "            featuresDict['lm']=lmFeatures\n",
    "            \n",
    "    #learning objectives\n",
    "    loFeatures=np.ones(loEdgeIndex.max()+1).reshape(-1,1)\n",
    "    featuresDict['lo']=loFeatures\n",
    "    \n",
    "    #target features\n",
    "    targetFeatures = userDf[userDf.atom_type=='target'][['difficulty']].to_numpy()\n",
    "    featuresDict['target']=targetFeatures\n",
    "    targetCorrectness=userDf[userDf.atom_type=='target'][['correctBinary']].astype(float).to_numpy()\n",
    "    \n",
    "    userDf_shift= userDf[~userDf.atomIndex_shift.isna()].copy() \n",
    "\n",
    "    edgesDict={}\n",
    "    #atom-atom\n",
    "    atomEdgeIndex=userDf_shift[(userDf_shift.atom_type=='question') & (userDf_shift.atom_type_shift=='question')][['atomIndex','atomIndex_shift']].values.transpose()\n",
    "    if atomEdgeIndex.size >0:\n",
    "        edgesDict['atom']=atomEdgeIndex\n",
    "    #atom-lmatomLmEdgeIndex\n",
    "    atomLmEdgeIndex=userDf_shift[(userDf_shift.atom_type=='question') & (userDf_shift.atom_type_shift=='learningMaterial')][['atomIndex','atomIndex_shift']].values.transpose()\n",
    "    if atomLmEdgeIndex.size>0:\n",
    "        edgesDict['atom_lm']=atomLmEdgeIndex\n",
    "    #lm-atom\n",
    "    LmAtomEdgeIndex=userDf_shift[(userDf_shift.atom_type=='learningMaterial') & (userDf_shift.atom_type_shift=='question')][['atomIndex','atomIndex_shift']].values.transpose()\n",
    "    if LmAtomEdgeIndex.size>0:\n",
    "        edgesDict['lm_atom']=LmAtomEdgeIndex\n",
    "    #lm-lm\n",
    "    LmLmEdgeIndex=userDf_shift[(userDf_shift.atom_type=='learningMaterial') & (userDf_shift.atom_type_shift=='learningMaterial')][['atomIndex','atomIndex_shift']].values.transpose()\n",
    "    if LmLmEdgeIndex.size>0:\n",
    "        edgesDict['lm_lm']=LmLmEdgeIndex\n",
    "    \n",
    "    #lm-target\n",
    "    LmTargetEdgeIndex=userDf_shift[(userDf_shift.atom_type=='learningMaterial') & (userDf_shift.atom_type_shift=='target')][['atomIndex','atomIndex_shift']].values.transpose()\n",
    "    if LmTargetEdgeIndex.size>0:\n",
    "        edgesDict['lm_target']=LmTargetEdgeIndex\n",
    "    \n",
    "    #atom-target\n",
    "    atomTargetEdgeIndex=userDf_shift[(userDf_shift.atom_type=='question') & (userDf_shift.atom_type_shift=='target')][['atomIndex','atomIndex_shift']].values.transpose()\n",
    "    \n",
    "    edgesDict['atom_target']=atomTargetEdgeIndex\n",
    "\n",
    "    if  loLmEdgeIndex.size>0:\n",
    "        edgesDict['lo_lm']=loLmEdgeIndex\n",
    "\n",
    "    if  loAtomEdgeIndex.size>0:\n",
    "        edgesDict['lo_atom']=loAtomEdgeIndex\n",
    "    \n",
    "    edgesDict['lo_target']=loTargetEdgeIndex\n",
    "    edgesDict['lo']=loEdgeIndex\n",
    "    return featuresDict,edgesDict,targetCorrectness,userDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582e6d7-6002-46f2-b5ab-43195534ba24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "f1b9f1a2-f160-40a5-8d46-ce80797a1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,df2,user,goal):\n",
    "        loEdgeMapping=df2[df2['graph_id']==goal][['source_index','dest_index']]\n",
    "        loEdgeIndex=loEdgeMapping.values.transpose()\n",
    "      \n",
    "        featuresDict,edgesDict,targetCorrectness,userDf=user_dataset_with_target_node(df,df2,user,goal,loEdgeIndex)\n",
    "        \n",
    "        return featuresDict,edgesDict,targetCorrectness,userDf\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42854daf-951d-4ef7-9a98-6a9b002fb440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "85a58937-ed00-4d2d-a0fc-9e754d5e26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_target(features,edges,targetCorrectness):  \n",
    "    data = HeteroData()\n",
    "    #node features\n",
    "    data['atom'].x=torch.tensor(features['atom'].astype(np.float64)).float()\n",
    "    data['lo'].x= torch.tensor(features['lo']).float()\n",
    "    data['target'].x= torch.tensor(features['target']).float()\n",
    "    \n",
    "    #edge index\n",
    "    data['lo','lo_atom','atom'].edge_index=torch.from_numpy(edges['lo_atom']).long()\n",
    "    data['lo','lo_lo','lo'].edge_index= torch.from_numpy(edges['lo']).long()\n",
    "    data['lo','lo_target','target'].edge_index=torch.from_numpy(edges['lo_target']).long()\n",
    "    data['atom','atom_atom','atom'].edge_index=torch.from_numpy(edges['atom']).long()\n",
    "    data['atom','atom_target','target'].edge_index=torch.from_numpy(edges['atom_target']).long()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #lm edges and features\n",
    "    if 'lm' in features:\n",
    "        data['lm'].x= torch.tensor(features['lm']).float()\n",
    "        data['lo','lo_lm','lm'].edge_index=torch.from_numpy(edges['lo_lm']).long()\n",
    "        \n",
    "    if 'lm_lm' in features:    \n",
    "        data['lm','lm_lm','lm'].edge_index=torch.from_numpy(edges['lm_lm']).long()\n",
    "    \n",
    "    if 'atom_lm' in features:\n",
    "        data['atom','atom_lm','lm'].edge_index=torch.from_numpy(edges['atom_lm']).long()\n",
    "    \n",
    "    if 'lm_atom' in features:\n",
    "        data['lm','atom_lm','atom'].edge_index=torch.from_numpy(edges['lm_atom']).long()\n",
    "    \n",
    "    data['target'].y=torch.from_numpy(targetCorrectness).long()\n",
    "\n",
    "    #train_mask = torch.ones(attom_count, dtype=torch.bool)\n",
    "    #train_mask[target_atom] = False\n",
    "    #data['atom'].train_mask = train_mask\n",
    "\n",
    "    #normalize=T.NormalizeFeatures()\n",
    "    #data=normalize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990207c-39ad-4c90-b356-ffd83a60a8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83240593-7e9e-4cfd-ad23-377b5b74abe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "db75b252-46cc-47ec-be5b-c4ed1310f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_batch_generator(df_tu_list):\n",
    "    data_list=[]\n",
    "    for i in tqdm(range(len(df_tu_list))):\n",
    "        featuresDict,edgesDict,targetCorrectness,userDf=preprocess(df_tu_list[i].copy(),df2,user_t,loGraphID) \n",
    "        data=generate_data_with_target(featuresDict,edgesDict,targetCorrectness)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "c2364811-e6d3-4cc7-9aba-7d412b1d7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loGraphID=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "58105f28-9372-43d2-a947-b1c22c30f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=df[(df.loGraphID==loGraphID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "2ec59226-9550-4948-891f-9a2ca17ef7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "e4ac6b18-5423-4886-b7ee-a8fd2c54ca9f    23\n",
       "9c40062e-0719-4f9b-9d6a-bca4d37a2ce5    23\n",
       "e3bce254-2394-46f3-9053-8437e5b810a4    24\n",
       "3eb9212c-523a-49fb-a670-3a51ae1ff87e    25\n",
       "1014fea4-cd45-4b59-b903-4c59a4360fcb    26\n",
       "bbc510e7-6659-4092-bde3-96d10fa0e7e0    26\n",
       "92a51cf7-0396-4751-8ac0-7e2ec2b4f02f    27\n",
       "7e0f991c-23b3-49bb-bcc5-38c26263bf4b    27\n",
       "efb10a25-84d0-4e96-bd3d-762b24f1b0a1    28\n",
       "ab380c7a-306e-4f3e-adf6-425bf2833a84    29\n",
       "ef963abb-beda-4105-b142-72b6d0ca8b6f    29\n",
       "c7e39c9c-39dc-42bd-a4ee-c622edecf3c9    32\n",
       "8259fae9-a618-46b6-a95a-380a41812f7b    34\n",
       "81f01061-9f19-4a2e-90b2-04c95bae81f2    34\n",
       "1c991ad0-89aa-4aa9-94b9-fea443b609bc    35\n",
       "Name: atom_id, dtype: int64"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.groupby('user_id')['atom_id'].count().sort_values()[25:40]#.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "d11cf688-abaa-4d8a-8521-fa92c0ce7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_t=dft.groupby('user_id')['atom_id'].count().sort_values()[25:50].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "86dd589b-004c-4691-a1f5-ef95eced4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tu=dft[dft.user_id.isin(user_t)].sort_values('interaction_end_time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "e1338f2f-6963-4770-8a94-5be9d18972fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tu_list=[]\n",
    "question_count=0\n",
    "for row_id,row in df_tu.iterrows():\n",
    "    if row['atom_type'] =='question':\n",
    "        question_count =question_count+1\n",
    "        if question_count>3:\n",
    "            df_tu_list.append(df_tu.iloc[:row_id+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "a2938f15-2b95-473b-a8a3-fdeb24f3e7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 626/626 [00:17<00:00, 36.55it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list=data_batch_generator(df_tu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "3719ef37-ad20-4172-a2fb-67bb571c3973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "801e9ab2-5c5a-495d-8b1c-b253f477f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141f28e-5c9b-4db2-a0ee-fbc0b99b55e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "59b4f867-58dc-45b5-8bb2-ff10f72d7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv, Linear, to_hetero\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "        self.lin1 = Linear(-1, hidden_channels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv2 = GATConv((-1, -1), out_channels, add_self_loops=False)\n",
    "        self.lin2 = Linear(-1, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
    "        x=x.sigmoid()\n",
    "        \n",
    "        return x\n",
    "\n",
    "#\n",
    "#model_2 = GAT(hidden_channels=64, out_channels=2)\n",
    "#model_2 = to_hetero(model_2, data.metadata(), aggr='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "90f90d2b-3fb8-41a0-bbab-d6221762f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "89a81c0a-0485-4aa3-b0c4-dcfa40fec183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1matom\u001b[0m={ x=[628, 2] },\n",
       "  \u001b[1mlo\u001b[0m={ x=[31, 1] },\n",
       "  \u001b[1mtarget\u001b[0m={\n",
       "    x=[1, 1],\n",
       "    y=[1, 1]\n",
       "  },\n",
       "  \u001b[1mlm\u001b[0m={ x=[176, 1] },\n",
       "  \u001b[1m(lo, lo_atom, atom)\u001b[0m={ edge_index=[2, 628] },\n",
       "  \u001b[1m(lo, lo_lo, lo)\u001b[0m={ edge_index=[2, 41] },\n",
       "  \u001b[1m(lo, lo_target, target)\u001b[0m={ edge_index=[2, 1] },\n",
       "  \u001b[1m(atom, atom_atom, atom)\u001b[0m={ edge_index=[2, 475] },\n",
       "  \u001b[1m(atom, atom_target, target)\u001b[0m={ edge_index=[2, 1] },\n",
       "  \u001b[1m(lo, lo_lm, lm)\u001b[0m={ edge_index=[2, 176] }\n",
       ")"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123d7e7-9642-4a5f-a379-9fe47d9b8aac",
   "metadata": {},
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "\n",
    "\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('lo', 'lo_lo', 'lo'): GCNConv(-1, hidden_channels),\n",
    "                ('atom', 'atom_atom', 'atom'): GCNConv(-1, hidden_channels),\n",
    "                ('atom', 'atom_target', 'target'): GCNConv(-1,  hidden_channels,add_self_loops=True),\n",
    "                ('lo', 'lo_atom', 'atom'): GCNConv(-1,  hidden_channels,add_self_loops=True),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['target'])\n",
    "\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "650155a5-26d0-4743-abc0-9da802b64286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom': 2, 'lo': 1, 'target': 1, 'lm': 1}"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data_list[-1].num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "53cb4366-7d90-4731-a3ac-66a958f45cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels=200\n",
    "out_channels=2\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels,requires_grad=False)\n",
    "        self.actfn1=nn.ReLU()\n",
    "        \n",
    "        self.conv2 = SAGEConv((-1, -1), hidden_channels,requires_grad=False)\n",
    "        self.actfn2=nn.ReLU()\n",
    "        \n",
    "        self.conv3 = SAGEConv((-1, -1), hidden_channels,requires_grad=False)\n",
    "        self.actfn3=nn.Sigmoid()\n",
    "        \n",
    "        self.out=nn.Linear(hidden_channels,out_channels)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        # First Message Passing layer\n",
    "        x = self.actfn1(self.conv1(x, edge_index))\n",
    "        x= F.dropout(x,p=.5,training=self.training)\n",
    "        \n",
    "        x = self.actfn2(self.conv2(x, edge_index))\n",
    "        x= F.dropout(x,p=.5,training=self.training)\n",
    "        #pool={key: gmp(batch[key].x, batch[key].batch) for key in batch.node_types}\n",
    "        out=self.sigmoid(self.out(x))\n",
    "        \n",
    "        return out,x\n",
    "\n",
    "\n",
    "#model_1 = GNN(hidden_channels=64, out_channels=2)\n",
    "#model_1 = to_hetero(model_1, data.metadata(), aggr='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "8a32cc87-d6da-4684-a843-01d20285710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "a8889873-6c4f-46f6-b589-f86aa4df7051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpathirana\\Anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "NUM_GRAPHS_PER_BATCH=10\n",
    "data_size=len(data_list)\n",
    "loader = DataLoader(data_list[:int(data_size * 0.8)], \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False)\n",
    "test_loader = DataLoader(data_list[int(data_size * 0.8):], \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c48e33-2f8f-4946-87f1-e70449c3d637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "257e2d47-5aec-45b1-8b39-2ed88aea10b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]2023-10-17 11:08:17 |3411507840.py:48 in <module>\n",
      "                     np.mean(epoch_accuracy): 0.6000000031292438\n",
      "                     loss.item(): 0.5132616758346558\n",
      "  5%|████▏                                                                              | 1/20 [01:04<20:23, 64.38s/it]2023-10-17 11:09:13 |3411507840.py:48 in <module>\n",
      "                     np.mean(epoch_accuracy): 0.6080000026524067\n",
      "                     loss.item(): 0.5132616758346558\n",
      " 10%|████████▎                                                                          | 2/20 [01:59<17:44, 59.15s/it]2023-10-17 11:10:09 |3411507840.py:48 in <module>\n",
      "                     np.mean(epoch_accuracy): 0.6080000026524067\n",
      "                     loss.item(): 0.5132616758346558\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [03:23<00:00, 10.18s/it]\n"
     ]
    }
   ],
   "source": [
    "#model = GNN(hidden_channels=200, out_channels=2)\n",
    "#model = to_hetero(model, data_list[-1].metadata())\n",
    "\n",
    "model=GNN()\n",
    "model = to_hetero(model, batch.metadata())\n",
    "learning_rate=0.01\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "#criterion=F.binary_cross_entropy\n",
    "train_acc_mean_list=[]\n",
    "test_acc_mean_list=[]\n",
    "loss_list=[]\n",
    "best_score=0\n",
    "\n",
    "train_size=int(len(data_list)*.8)\n",
    "train_data=data_list[:train_size]\n",
    "test_data=data_list[train_size:]\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(20)):\n",
    "    epoch_accuracy=[]\n",
    "    try:\n",
    "        for  batch_num,batch in enumerate(loader):\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()   \n",
    "            #x_dict = {key: gmp(batch[key].x, batch[key].batch) for key in batch.node_types}\n",
    "            out,embedding=model(batch.x_dict, batch.edge_index_dict) \n",
    "\n",
    "            loss=criterion(out['target'],batch['target'].y.view(1,-1)[0])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_correct=out['target'].argmax(dim=1)==batch['target'].y.view(1,-1)[0]\n",
    "\n",
    "            train_acc=train_correct*1\n",
    "\n",
    "            accuracy=(sum(train_correct*1)/train_correct.shape[0]).item()\n",
    "            epoch_accuracy.append(accuracy)\n",
    "        if i==0:\n",
    "            best_model=model\n",
    "            best_loss=loss\n",
    "        else:\n",
    "            if loss<best_loss:\n",
    "                  best_model=model  \n",
    "        ic(np.mean(epoch_accuracy),loss.item())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#out['target']: tensor([[0.2217, 0.0272]], grad_fn=<AddmmBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "18b48054-31d8-4583-9a89-9533ff303098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 11:10:42 |1129849738.py:12 in <module>- performance: '0.80'\n",
      "2023-10-17 11:10:44 |1129849738.py:12 in <module>- performance: '0.80'\n",
      "2023-10-17 11:10:46 |1129849738.py:12 in <module>- performance: '0.60'\n",
      "2023-10-17 11:10:49 |1129849738.py:12 in <module>- performance: '0.40'\n",
      "2023-10-17 11:10:51 |1129849738.py:12 in <module>- performance: '0.40'\n",
      "2023-10-17 11:10:54 |1129849738.py:12 in <module>- performance: '0.60'\n",
      "2023-10-17 11:10:56 |1129849738.py:12 in <module>- performance: '0.60'\n",
      "2023-10-17 11:11:00 |1129849738.py:15 in <module>\n",
      "                     np.mean(test_accuracy): 0.6000000153269086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6000000153269086"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy=[]\n",
    "for num, data in enumerate(test_loader):\n",
    "    try:\n",
    "        out,x=best_model(data.x_dict, data.edge_index_dict)\n",
    "        pred=out['target'].argmax(dim=1)\n",
    "        test_correct=out['target'].argmax(dim=1)==data['target'].y.view(1,-1)[0]\n",
    "        test_acc=test_correct*1\n",
    "        accuracy=(sum(test_correct*1)/test_correct.shape[0]).item()\n",
    "        performance='{:.2f}'.format(accuracy)\n",
    "        test_accuracy.append(accuracy)\n",
    "        ic(performance);\n",
    "    except:\n",
    "        pass\n",
    "ic(np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "00d997e0-fa29-4817-9b0f-9383ea0c4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=data_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "322d3679-ef58-46a4-a714-6719dbd0f598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1matom\u001b[0m={ x=[8, 2] },\n",
       "  \u001b[1mlo\u001b[0m={ x=[31, 1] },\n",
       "  \u001b[1mtarget\u001b[0m={\n",
       "    x=[1, 1],\n",
       "    y=[1, 1]\n",
       "  },\n",
       "  \u001b[1mlm\u001b[0m={ x=[3, 1] },\n",
       "  \u001b[1m(lo, lo_atom, atom)\u001b[0m={ edge_index=[2, 8] },\n",
       "  \u001b[1m(lo, lo_lo, lo)\u001b[0m={ edge_index=[2, 41] },\n",
       "  \u001b[1m(lo, lo_target, target)\u001b[0m={ edge_index=[2, 1] },\n",
       "  \u001b[1m(atom, atom_atom, atom)\u001b[0m={ edge_index=[2, 6] },\n",
       "  \u001b[1m(atom, atom_target, target)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(lo, lo_lm, lm)\u001b[0m={ edge_index=[2, 3] }\n",
       ")"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "95948adc-7433-4cfa-8a02-0343ca57fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "42be2ecc-5192-46dd-86ea-5d188081b337",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'HeteroData' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[827], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpyg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch_geometric\\utils\\convert.py:132\u001b[0m, in \u001b[0;36mto_networkx\u001b[1;34m(data, node_attrs, edge_attrs, graph_attrs, to_undirected, remove_self_loops)\u001b[0m\n\u001b[0;32m    129\u001b[0m graph_attrs \u001b[38;5;241m=\u001b[39m graph_attrs \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    131\u001b[0m values \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode_attrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43medge_attrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgraph_attrs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(value):\n\u001b[0;32m    134\u001b[0m         value \u001b[38;5;241m=\u001b[39m value \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m value\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'HeteroData' object is not callable"
     ]
    }
   ],
   "source": [
    "pyg.utils.to_networkx(sample.to_homogeneous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526f916-1edd-4121-b804-9a24724345b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
