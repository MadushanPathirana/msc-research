{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29466b25-846b-42e8-a9a7-477608cd4987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13b8ca-5d9b-436e-84eb-71224ec6ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels=64\n",
    "final_layer_dim=hidden_channels/4\n",
    "class GNN_Lstm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels,requires_grad=True)\n",
    "        self.actfn1=nn.ReLU()\n",
    "        \n",
    "        self.conv2 = SAGEConv((-1, -1),int( hidden_channels/2),requires_grad=True)\n",
    "        self.actfn2=nn.ReLU()\n",
    "        \n",
    "        self.conv3 = SAGEConv((-1, -1),int( final_layer_dim),requires_grad=True)\n",
    "        self.actfn3=nn.ReLU()\n",
    "        \n",
    "        self.conv4 = SAGEConv((-1, -1),int( hidden_channels/8),requires_grad=True)\n",
    "        self.actfn4=nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.sigmoid= nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        x = self.actfn1(self.conv1(x, edge_index))\n",
    "        #x= F.dropout(x,p=.5,training=self.training)\n",
    "        \n",
    "        x = self.actfn2(self.conv2(x, edge_index))\n",
    "        #x= F.dropout(x,p=.7,training=self.training)\n",
    "        \n",
    "        x = self.actfn3(self.conv3(x, edge_index))\n",
    "        \n",
    "        #out = self.actfn4(self.conv4(x, edge_index))\n",
    "        \n",
    "        #out= F.dropout(x,p=.7,training=self.training)\n",
    "        #pool={key: gmp(out[key].x, out[key].batch) for key in out.node_types}\n",
    "        #out=self.sigmoid( self.out(x))\n",
    "        #ic(pool)\n",
    "        \n",
    "        #pool=gmp(out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6c7d3-6182-48bc-9937-d4b4ba78cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback=3\n",
    "input_size=33\n",
    "out_channels=1\n",
    "num_layers=1\n",
    "hidden_size=20\n",
    "class Classifer_Lstm(nn.Module):\n",
    "    \n",
    "    def __init__(self ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size # input size\n",
    "        self.hidden_size = hidden_size # neurons in each lstm layer\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size ,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, 1, self.hidden_size).requires_grad_()\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(self.num_layers, 1, self.hidden_size).requires_grad_()\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        #print(out.shape)\n",
    "        out = self.sigmoid(self.fc(out[:, -1, :]))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55c1a9-e1c6-4497-b318-11c53a2356b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_LSTM_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN_LSTM_Classifier, self).__init__()\n",
    "        \n",
    "        self.model_embedding=GNN_Lstm()\n",
    "        self.model_embedding = to_hetero(self.model_embedding, data.metadata(),aggr='max')\n",
    "        self.classifer = Classifer_Lstm()\n",
    "        #lo,atom,prev_answer,prev_lo,prev_atom,prev_answer1,prev_lo1,prev_atom1\n",
    "    def forward(self, x,edges,batch_num,lookback,question_sequence):\n",
    "        node_embeddings = self.model_embedding(x,edges)\n",
    "        rnn_input_list=[]\n",
    "        #features_concat_list=[]\n",
    "        \n",
    "        for i in range(lookback+1):\n",
    "            \n",
    "            #print([i,batch_num-i])\n",
    "            lo=question_sequence[0][batch_num-i]\n",
    "            atom=question_sequence[1][batch_num-i]\n",
    "            prev_answer=torch.tensor([question_sequence[2][batch_num-1-i]]).float()\n",
    "\n",
    "            features_concat=torch.cat([node_embeddings['atom'][atom],node_embeddings['lo'][lo],prev_answer])\n",
    "            rnn_input_list.append(features_concat)\n",
    "        \n",
    "                                         \n",
    "            \n",
    "        #user_embeddings=torch.cat([node_embeddings['atom'][atom] , node_embeddings['lo'][lo],prev_answer,node_embeddings['atom'][prev_atom] , node_embeddings['lo'][prev_lo],prev_answer1,node_embeddings['atom'][prev_atom1] , node_embeddings['lo'][prev_lo1]])  \n",
    "       \n",
    "        rnn_input_stacked=torch.stack(rnn_input_list).view(1,lookback+1,features_concat.shape[0])\n",
    "\n",
    "        rnn_outputs = self.classifer(rnn_input_stacked)\n",
    "    \n",
    "        return rnn_outputs,node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82179d-f5ab-4a46-9c58-8e726dfccca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size=sum([users_question_sequence_train_lists[i].shape[1] for i in range(len(users_question_sequence_train_lists))])\n",
    "\n",
    "model=GNN_LSTM_Classifier()\n",
    "learning_rate=0.001\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate,weight_decay=5e-12)\n",
    "criterion=torch.nn.BCELoss()\n",
    "#criterion=F.binary_cross_entropy\n",
    "train_acc_mean_list=[]\n",
    "test_acc_mean_list=[]\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "best_score=0\n",
    "\n",
    "torch.manual_seed(42)\n",
    "loss_moving_avg=[]\n",
    "user_embedings=[]\n",
    "for epoch_num in range(2):\n",
    "    epoch_accuracy=[]\n",
    "    out_list=[]\n",
    "    total_loss=0\n",
    "    \n",
    "    prob=[]\n",
    "    target=[]\n",
    "    for user_count,question_sequence in enumerate(users_question_sequence_train_lists):\n",
    "        question_sequence_len=question_sequence.shape[1]\n",
    "        batch_loss=0\n",
    "        for  batch_num in range(lookback,question_sequence_len):\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()   \n",
    "            #x_dict = {key: gmp(batch[key].x, batch[key].batch) for key in batch.node_types}\n",
    "\n",
    "\n",
    "            answer=torch.tensor([question_sequence[2][batch_num]]).float()\n",
    "\n",
    "\n",
    "            out,node_embeddings=model(data.x_dict, data.edge_index_dict,batch_num,lookback,question_sequence) \n",
    "\n",
    "            loss=F.binary_cross_entropy(out.view(1),answer)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            total_loss=total_loss+loss.item()\n",
    "            train_correct=out.round()==answer\n",
    "\n",
    "            train_acc=train_correct*1\n",
    "            accuracy=(train_correct*1).item()\n",
    "            epoch_accuracy.append(accuracy)\n",
    "            #accuracy_list.append(accuracy)\n",
    "            prob.append(out.item())\n",
    "            target.append(answer.item())\n",
    "            batch_loss=batch_loss+loss.item()\n",
    "        print('epoch {} | batch {} | batch loss {:.5f}'.format(epoch_num,user_count+1,batch_loss/question_sequence_len))\n",
    "    \n",
    "    performance='epoch {} | loss {:.5f} | accuracy {:.5f} | auc {:.5f} \\n'.format(epoch_num,total_loss/(training_size),np.mean(epoch_accuracy),roc_auc_score(np.array(target),prob))\n",
    "    print('-'*len(performance))\n",
    "    print(performance)\n",
    "    \n",
    "    loss_moving_avg.append(total_loss/(batch_num+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570c71c-0e86-42ab-a82e-a7ff07778498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    for user_count,question_sequence in enumerate(users_question_sequence_test_lists):\n",
    "            question_sequence_len=question_sequence.shape[1]\n",
    "            batch_accuracy=[]\n",
    "            batch_target=[]\n",
    "            batch_prob=[]\n",
    "            for  batch_num in range(lookback,question_sequence_len):\n",
    "\n",
    "                model.eval()\n",
    "                answer=torch.tensor([question_sequence[2][batch_num]]).float()\n",
    "\n",
    "                out,node_embeddings=model(data.x_dict, data.edge_index_dict,batch_num,lookback,question_sequence) \n",
    "\n",
    "                train_correct=out.round()==answer\n",
    "\n",
    "                train_acc=train_correct*1\n",
    "                accuracy=(train_correct*1).item()\n",
    "                epoch_accuracy.append(accuracy)\n",
    "                prob.append(out.item())\n",
    "                target.append(answer.item())\n",
    "                batch_target.append(answer.item())\n",
    "                batch_prob.append(out.item() )\n",
    "                batch_accuracy.append(accuracy)\n",
    "            batch_performance='accuracy {:.5f} | auc {:.5f} \\n'.format(np.mean(batch_accuracy),roc_auc_score(np.array(batch_target),batch_prob))\n",
    "                \n",
    "            print(batch_performance)\n",
    "      \n",
    "    performance='accuracy {:.5f} | auc {:.5f} \\n'.format(np.mean(epoch_accuracy),roc_auc_score(np.array(target),prob))\n",
    "    print('-'*len(performance))\n",
    "    print(performance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
